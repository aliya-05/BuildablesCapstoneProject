{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a93f53-3de6-4cd3-a92a-e740c1ded597",
   "metadata": {},
   "source": [
    "**Stage 3 - Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b6456b-cb64-4911-ab5c-bb1fd7b8656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6211ea45-ba25-4a1e-8f99-b02e75af4521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset length: 187469564\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and load files\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\Aliya Sarfaraz\\OneDrive\\Desktop\\Softwares\\git_github\\100DaysOfBuildables\\Capstone-Project\"\n",
    "TEXT_FILE = os.path.join(BASE_DIR, \"pile_uncopyrighted_100k.txt\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"char_lstm_model.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "with open(TEXT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Dataset length:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56de4d15-b652-4721-b69e-b9e7784e5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 152\n"
     ]
    }
   ],
   "source": [
    "# 2. Build vocab from data\n",
    "\n",
    "itos = sorted(list(set(text)))\n",
    "stoi = {ch: i for i, ch in enumerate(itos)}\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def encode(s):  # text -> integers\n",
    "    return [stoi[c] for c in s if c in stoi]\n",
    "\n",
    "def decode(l):  # integers -> text\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9de1f48-97b8-4464-8b98-be6a3cb0d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare training data\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34ddc83-f84a-4020-a50a-c5f69ba59a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define model\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "model = CharLSTM(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893276e1-a952-4ae9-b72f-211689bf35fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 1/25, Loss: 2.2054\n",
      "Epoch 2/25, Loss: 1.7260\n",
      "Epoch 3/25, Loss: 1.5868\n"
     ]
    }
   ],
   "source": [
    "# 5. Train model\n",
    "\n",
    "epochs = 25  # you can increase later for better results\n",
    "print(\"Training started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _ in range(200):  # mini-epochs\n",
    "        x, y = get_batch()\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(x)\n",
    "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/200:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(\"Model trained and saved to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9271f-135b-4144-a584-e2393305a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Text generation function \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"Ä \", \" \")\n",
    "    text = text.replace(\" .\", \".\").replace(\" ,\", \",\")\n",
    "    text = ' '.join(text.split())  # remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "def generate_text(model, start_text=\"Once upon a time\", length=400, temperature=0.5):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    generated = list(start_text)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            logits = output[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1).detach().cpu().numpy().ravel()\n",
    "            next_idx = np.random.choice(len(probs), p=probs)\n",
    "            next_char = itos[next_idx]\n",
    "            generated.append(next_char)\n",
    "            input_seq = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return clean_text(''.join(generated))\n",
    "\n",
    "# Example usage\n",
    "text_out = generate_text(model, start_text=\"Once upon a time\", length=400, temperature=0.5)\n",
    "print(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa2541-71cf-4bec-bd2c-894f5e9178d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate examples and save results\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The future of AI\",\n",
    "    \"In a small village\",\n",
    "    \"Medical technology\",\n",
    "    \"Sports injuries\"\n",
    "]\n",
    "\n",
    "examples = []\n",
    "for prompt in prompts:\n",
    "    text_out = generate_text(model, start_text=prompt, length=300, temperature=0.7)\n",
    "    text_out = clean_text(text_out)\n",
    "    examples.append({\"prompt\": prompt, \"generated_text\": text_out})\n",
    "\n",
    "# Save results\n",
    "output_file = os.path.join(BASE_DIR, \"evaluation_outputs.csv\")\n",
    "df = pd.DataFrame(examples)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Generated texts saved to {output_file}\")\n",
    "\n",
    "# Print an example \n",
    "print(\"\\nExample generated text for prompt:\", prompts[0])\n",
    "print(examples[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
