{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23a9d83",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cbbcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.3.4\n",
      "Uninstalling numpy-2.3.4:\n",
      "  Successfully uninstalled numpy-2.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\~~mpy.libs'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\~~mpy'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d00c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/15.5 MB 3.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.4/15.5 MB 5.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.1/15.5 MB 4.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/15.5 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.0/15.5 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.5/15.5 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.8/15.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.5 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.5 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.5 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.2/15.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.7/15.5 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.0/15.5 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.5/15.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.0/15.5 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.3/15.5 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.8/15.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.3/15.5 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.6/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.1/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.4/15.5 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 2.7 MB/s  0:00:05\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.45.0 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 --force-reinstall --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5acfaac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "PyTorch version: 2.2.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "print(\"NumPy version:\", numpy.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c3cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Temp\\ipykernel_16344\\188653193.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "C:\\Users\\ma007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Model Development - Text Generation using LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(__file__)\n",
    "TEXT_FILE = os.path.join(BASE_DIR, \"pile_uncopyrighted_50MB.txt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc4d9d",
   "metadata": {},
   "source": [
    "# Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text length: 500,000 characters\n",
      "Unique characters: 91\n"
     ]
    }
   ],
   "source": [
    "#  LOADING DATA \n",
    "with open(TEXT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "text = text[:500000]\n",
    "print(f\"Loaded text length: {len(text):,} characters\")\n",
    "\n",
    "#  CREATING CHAR MAPPING \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Unique characters:\", vocab_size)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "def encode(s): \n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l): \n",
    "    return ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5efb5c",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DATASET \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=100):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx+self.seq_len+1]\n",
    "        input_seq = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        target_seq = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return input_seq, target_seq\n",
    "\n",
    "encoded = encode(text)\n",
    "dataset = TextDataset(encoded, seq_len=100)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953706c7",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MODEL \n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "model = CharLSTM(vocab_size=vocab_size, embed_size=128, hidden_size=256, num_layers=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef2346",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219eb4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Loss: 0.9473\n",
      "Epoch [2/5] - Loss: 0.5545\n",
      "Epoch [3/5] - Loss: 0.4718\n",
      "Epoch [4/5] - Loss: 0.4387\n",
      "Epoch [5/5] - Loss: 0.4194\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "EPOCHS = 5  \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(inputs)\n",
    "        loss = criterion(output.transpose(1, 2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f859912",
   "metadata": {},
   "source": [
    "# Text Generalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time Ä h u , Ä pod isag Ä gave , Ä business Ä were Ä failured Ä format . Ä you Ä can Ä but Ä by Ä map . Ä these Ä manner Ä while Ä dis ac char ides Ä and Ä his Ä coached Ä with Ä a Ä test Ä careful Ä by Ä using Ä micro soft Ä test Ä manager Ä creates Ä and Ä man gard . Ä we Ä have Ä of Ä measures Ä were Ä revealed . Ä success Ä and Ä markup Ä molder Ä from Ä home Ä in Ä the Ä fr c Ä noted Ä that Ä 50 Ä of Ä k pm gs Ä ft se Ä 350 Ä audits Ä failed Ä to Ä remove Ä be ijing Ä phbillar Ä prosect Ä no . Ä the Ä 12 pot ty es Ä  Ä t Ä t Ä t j . Ä get Ä tot al ies Ä marker Ä s\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text=\"hello\", length=300):\n",
    "    model.eval()\n",
    "    start_text = start_text.lower()  #  conversion to lowercase\n",
    "    input_seq = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    generated = list(start_text)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            last_logits = output[0, -1]\n",
    "            probs = torch.nn.functional.softmax(last_logits, dim=0)\n",
    "            idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(itos[idx])\n",
    "            input_seq = torch.tensor([[idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return ''.join(generated)\n",
    "print(generate_text(model, start_text=\"Once upon a time\", length=500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ec7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time Ä hyp ox amenied Ä of Ä html wont Ä defenced Ä envelope Ä and Ä gravity Ä waves Ä in Ä the Ä sun Ä major Ä on Ä our Ä petition Ä for Ä disturbances Ä such Ä as Ä the Ä an is otide Ä function . Ä what Ä do Ä you Ä think Ä is Ä also Ä often Ä used Ä to Ä increase Ä herogid Ä type Ä with Ä servers , Ä including Ä was Ä because Ä it Ä notes Ä or Ä oil Ä hy row Ä for Ä intervening Ä denical Ä hardly , Ä the Ä output Ä file , Ä washing ton , Ä d 0 d 15 a Ä  Ä inc ., Ä 51 Ä fr and i od\n",
      "party Ä the Ä isolated Ä nuscle Ä center Ä in Ä recent Ä hydrogen Ä bygara Ä to Ä \n"
     ]
    }
   ],
   "source": [
    "#  GENERATE TEXT \n",
    "def generate_text(model, start_text=\"hello\", length=300):\n",
    "    model.eval()\n",
    "    start_text = start_text.lower()   # ðŸ”¥ convert to lowercase to match vocab\n",
    "    input_seq = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    generated = list(start_text)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            probs = torch.softmax(output[:, -1, :], dim=-1).detach().cpu().numpy().ravel()\n",
    "            next_idx = np.random.choice(len(probs), p=probs)\n",
    "            next_char = itos[next_idx]\n",
    "            generated.append(next_char)\n",
    "            input_seq = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "# Test after training\n",
    "print(generate_text(model, start_text=\"Once upon a time\", length=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853f417",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752545c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at C:\\Users\\ma007\\OneDrive\\Desktop\\Buildables_Project3\\char_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = os.path.join(BASE_DIR, \"char_lstm_model.pth\")\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved at {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2d047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
