{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d453af86-8b2c-47fd-8a3e-3b422bab3e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded text length: 500,000 characters\n",
      "Unique characters: 91\n",
      "Epoch [1/5] - Loss: 0.9892\n",
      "Epoch [2/5] - Loss: 0.5925\n",
      "Epoch [3/5] - Loss: 0.5029\n",
      "Epoch [4/5] - Loss: 0.4657\n",
      "Epoch [5/5] - Loss: 0.4440\n",
      "once upon a time Ġrelationships Ġare Ġassembly Ġand Ġas Ġper Ġjava Ġw aylin Ġh bs ag Ġhas Ġbeen Ġdemonstrated Ġthe Ġeffective Ġsize Ġand Ġdirector Ġof Ġa Ġfew Ġshort Ġvirus Ġwere Ġseen Ġfor Ġthemselves Ġagain Ġand Ġib ib util . Ġthe Ġagreement Ġin Ġconnectors . Ġto Ġeman Ġof Ġknown Ġelfsole Ġk ogated Ġname Ġhas Ġcan Ġcaught Ġother Ġagent Ġand Ġadminisal Ġsetting Ġangle Ġleft Ġ5 Ġand Ġoff , Ġto Ġexearch Ġtell Ġwas Ġban Ġin Ġcomparison Ġto Ġthe Ġindication Ġof Ġboth Ġpath ogen Ġhim Ġdemonstrated Ġfor Ġsuch Ġas Ġc\n",
      "Model saved at .\\char_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \".\"\n",
    "TEXT_FILE = os.path.join(BASE_DIR, \"pile_uncopyrighted_50MB.txt\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"char_lstm_model.pth\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "with open(TEXT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:500000]  # limit for training demo\n",
    "print(f\"Loaded text length: {len(text):,} characters\")\n",
    "\n",
    "# Character mapping\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Unique characters:\", vocab_size)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=100):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx+self.seq_len+1]\n",
    "        input_seq = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        target_seq = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return input_seq, target_seq\n",
    "\n",
    "encoded = encode(text)\n",
    "dataset = TextDataset(encoded, seq_len=100)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "model = CharLSTM(vocab_size=vocab_size, embed_size=128, hidden_size=256, num_layers=2).to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(inputs)\n",
    "        loss = criterion(output.transpose(1, 2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Text generation function\n",
    "def generate_text(model, start_text=\"hello\", length=300):\n",
    "    model.eval()\n",
    "    start_text = start_text.lower()  # convert to lowercase\n",
    "    input_seq = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    generated = list(start_text)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            probs = torch.softmax(output[:, -1, :], dim=-1).detach().cpu().numpy().ravel()\n",
    "            next_idx = np.random.choice(len(probs), p=probs)\n",
    "            next_char = itos[next_idx]\n",
    "            generated.append(next_char)\n",
    "            input_seq = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "# Test generation\n",
    "print(generate_text(model, start_text=\"Once upon a time\", length=500))\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved at {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300497f-fb7e-4719-a8ee-ffdafbfc985c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
